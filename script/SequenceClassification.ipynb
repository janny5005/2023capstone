{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "psLAHX-ibM_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers~=2.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QarvIBQ2ao5X",
        "outputId": "5801097d-bb06-4f06-ebda-74be99de80a3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers~=2.11.0 in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (0.1.97)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (0.0.53)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (2022.6.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (3.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (1.22.4)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers~=2.11.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers~=2.11.0) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers~=2.11.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers~=2.11.0) (4.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers~=2.11.0) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers~=2.11.0) (8.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers~=2.11.0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjdiD93JbKhs",
        "outputId": "ccf759db-c263-43c4-cd24-c7866f62b59d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.8/dist-packages (0.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.22.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.26.82)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.13.1+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.5.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.82 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (1.29.82)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.82->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.82->boto3->pytorch_pretrained_bert) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULou06Z5bUGd",
        "outputId": "4224d465-3f42-40eb-f843-70233c2f27a2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Google Drive"
      ],
      "metadata": {
        "id": "MwswTFhymZtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH4pe4CLmZ4J",
        "outputId": "486cb9aa-79f3-4515-bce5-92ba65e0d110"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive\")"
      ],
      "metadata": {
        "id": "DepiXFaZmj95"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "J4pchGlNoovq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_path = './data/capstone/CLAWS/covidhate/annotated_tweets_w_text.csv'\n",
        "data_path = './data/capstone/CLAWS/covidhate/part_labeled_tweet.csv'\n",
        "#data_path = './data/part_labeled_tweet.csv'"
      ],
      "metadata": {
        "id": "DbhPwu1smlmS"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainedBertModel = 'bert'"
      ],
      "metadata": {
        "id": "Sm9AJzwkonKv"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batchsize = 8"
      ],
      "metadata": {
        "id": "vGo1KXEEo_W6"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_num = 0"
      ],
      "metadata": {
        "id": "1Ypeg_iP1GIX"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "4_XmfbDvbbRQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "4xFudb4HZv9v"
      },
      "outputs": [],
      "source": [
        "# Basic and System\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time, datetime, random, glob, os, sys, joblib, argparse, json\n",
        "# Torch\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, Subset\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "# Pretrained Models\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from tqdm import tqdm \n",
        "# Scheduler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "# OPtimizer\n",
        "from transformers import AdamW\n",
        "# Kfold Cross Validation\n",
        "from sklearn.model_selection import KFold\n",
        "# Evaluation\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Variables\n",
        "USING_GPU = False\n",
        "DEVICE = None"
      ],
      "metadata": {
        "id": "pOfyaHj7av2_"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained models dictionary\n",
        "# Try in the future: Albert\n",
        "pretrained_models = {'bert': 'bert-base-uncased', 'roberta': 'roberta-base'}"
      ],
      "metadata": {
        "id": "gTNRo7cnb-K3"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(seed_num)\n",
        "np.random.seed(seed_num)\n",
        "torch.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed_all(seed_num)"
      ],
      "metadata": {
        "id": "jY-AhH3N1IYB"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format time display\n",
        "def format_time(seconds):\n",
        "    seconds_round = int(round((seconds)))\n",
        "    return str(datetime.timedelta(seconds=seconds_round)) # hh:mm:ss"
      ],
      "metadata": {
        "id": "f5pCIOgyX8te"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for pytorch dataloader\n",
        "def prepare_dataset(sentences, labels, tokenizer, max_length=100):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        # print(sent)\n",
        "        try:\n",
        "            encoded_dict = tokenizer.encode_plus(\n",
        "                                sent,\n",
        "                                add_special_tokens = True,\n",
        "                                max_length = max_length,\n",
        "                                truncation=True,\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,\n",
        "                                return_tensors = 'pt'\n",
        "                           )\n",
        "        except:\n",
        "            print(\"some tweet sent is not correct\")\n",
        "            print(sent)\n",
        "            exit(0)\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    return input_ids, attention_masks, labels"
      ],
      "metadata": {
        "id": "K1oI7oXzcyRT"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(fold, model, device, train_loader, optimizer, scheduler, epoch):\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        print('step: '+str(step))\n",
        "        model.zero_grad()\n",
        "        \n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n",
        "        \n",
        "        b_input_ids = batch[0].to(DEVICE)\n",
        "        b_input_mask = batch[1].to(DEVICE)\n",
        "        b_labels = batch[2].unsqueeze(0).to(DEVICE)\n",
        "        #print(b_input_ids.shape)\n",
        "        #print(b_input_mask.shape)\n",
        "        #print(b_labels.shape)\n",
        "        # https://stackoverflow.com/questions/70548318/bertforsequenceclassification-target-size-torch-size1-16-must-be-the-same\n",
        "        #b_labels = torch.nn.functional.one_hot(b_labels.to(torch.int64), 3)\n",
        "        #print(type(b_labels))\n",
        "        #print(b_labels.shape)\n",
        "        #loss, logits, hidden_states = model(b_input_ids,\n",
        "        loss, logits = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "        #print('run more')\n",
        "        total_train_loss += loss.item()\n",
        "        #loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        del loss, logits\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))"
      ],
      "metadata": {
        "id": "042JRUkecyUC"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add following:\n",
        "validation accuracy\n"
      ],
      "metadata": {
        "id": "78t7T8rDp-PI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(fold, model, device, test_loader, test_data_len):\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    model.eval()\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    predictions, true_labels = [], []\n",
        "    for batch in tqdm(test_loader, total=test_data_len):\n",
        "        # b_labels are true labels\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch[0].to(DEVICE), token_type_ids=None,\n",
        "                            attention_mask=batch[1].to(DEVICE))\n",
        "            b_proba = outputs[0]\n",
        "            # Accumulate the validation loss.\n",
        "            #total_eval_loss += b_proba.item()\n",
        "\n",
        "            proba = b_proba.detach().cpu().numpy()\n",
        "            label_ids = batch[2].numpy()\n",
        "\n",
        "            predictions.append(proba)\n",
        "            true_labels.append(label_ids)\n",
        "\n",
        "            # Calculate the accuracy for this batch of test sentences, and\n",
        "            # accumulate it over all batches.\n",
        "            total_eval_accuracy += accuracy_score(label_ids, b_labels)\n",
        "\n",
        "    #print(predictions)\n",
        "    #print(true_labels)\n",
        "    #print(b_labels)"
      ],
      "metadata": {
        "id": "JGSJos5QcyWy"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper function for fine-tuning\n",
        "# batch size, epochs, and learning rate are from CLAWS paper\n",
        "def train_bert_model(model, dataset, batch_size, epochs=3, learning_rate=1e-5, epsilon=1e-8, save_fn=None):\n",
        "\n",
        "    if USING_GPU:\n",
        "        print(\"Using GPU\", DEVICE)\n",
        "        model.cuda(DEVICE)\n",
        "\n",
        "    # prepare cross validation\n",
        "    n_fold = 5\n",
        "    kfold = KFold(n_splits=n_fold, shuffle=True)\n",
        "\n",
        "    n_accuracy = []\n",
        "    n_prescision = []\n",
        "    n_recall = []\n",
        "    n_f1 = []\n",
        "\n",
        "    # for each fold\n",
        "    for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset)):\n",
        "        print()\n",
        "        print('------------fold no---------{}----------------------'.format(fold))\n",
        "        #print(train_idx)\n",
        "        #print(test_idx)\n",
        "        train_tensor = Subset(dataset, train_idx)\n",
        "        test_tensor = Subset(dataset, test_idx)\n",
        "        test_data_len = len(test_idx)\n",
        "\n",
        "        trainloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=RandomSampler(train_tensor))\n",
        "        testloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=SequentialSampler(test_tensor))\n",
        "        \n",
        "        total_steps = len(trainloader) * epochs\n",
        "        optimizer = AdamW(model.parameters(),\n",
        "                          lr=learning_rate,\n",
        "                          eps=epsilon\n",
        "                          )\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                               num_warmup_steps=0,  # Default value in run_glue.py\n",
        "                               num_training_steps=total_steps)\n",
        "        \n",
        "        for epoch_i in range(0, epochs):\n",
        "            print()\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            train(fold, model, DEVICE, trainloader, optimizer, scheduler, epochs)\n",
        "            test(fold, model, DEVICE, testloader, test_data_len)\n"
      ],
      "metadata": {
        "id": "4jE2K2IpcyZk"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict data\n",
        "def run_bert_model(model, test_dataset, batch_size):    \n",
        "    print('Predicting labels for {:,} test sentences...'.format(len(test_dataset)))\n",
        "    \n",
        "    if USING_GPU:\n",
        "        print(\"Using GPU\", DEVICE)\n",
        "        model.cuda(DEVICE)\n",
        "\n",
        "    model.eval()\n",
        "    predictions , true_labels = [], []\n",
        "    prediction_sampler = SequentialSampler(test_dataset)\n",
        "    prediction_dataloader = DataLoader(test_dataset, sampler=prediction_sampler, batch_size=batch_size)\n",
        "    for batch in tqdm(prediction_dataloader, total=len(test_dataset)):\n",
        "        \n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch[0].to(DEVICE), token_type_ids=None,\n",
        "                                  attention_mask=batch[1].to(DEVICE))\n",
        "            b_proba = outputs[0]\n",
        "\n",
        "            proba = b_proba.detach().cpu().numpy()\n",
        "            label_ids = batch[2].numpy()\n",
        "\n",
        "            predictions.append(proba)\n",
        "            true_labels.append(label_ids)\n",
        "\n",
        "    print('    DONE.')\n",
        "   \n",
        "    flat_predictions = np.concatenate(predictions, axis=0)\n",
        "    return flat_predictions"
      ],
      "metadata": {
        "id": "4SPuk_csh0tC"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    USING_GPU = True\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    USING_GPU = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaAtAq-hcycH",
        "outputId": "129c5cd3-d2d9-4e78-fadc-ec6202df0543"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(data_path)\n",
        "df = df[df['Text'].notna()]\n",
        "X = df.Text.values # x\n",
        "Y = list(df['label']) # y_true"
      ],
      "metadata": {
        "id": "mPy1HwuQoWf2"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "LzKnryqSyJiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if trainedBertModel == 'bert':\n",
        "    used_bert_model = pretrained_models[trainedBertModel]\n",
        "    model = BertForSequenceClassification.from_pretrained(used_bert_model, num_labels = 3)\n",
        "    tokenizer = BertTokenizer.from_pretrained(used_bert_model, do_lower_case=True)\n",
        "elif trainedBertModel == 'roberta':\n",
        "    used_bert_model = pretrained_models[trainedBertModel]\n",
        "    model = RobertaForSequenceClassification.from_pretrained(used_bert_model, num_labels = 3)\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(used_bert_model, do_lower_case=True)\n",
        "\n",
        "input_ids, attention_masks, labels = prepare_dataset(X, Y, tokenizer, max_length=400)\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)"
      ],
      "metadata": {
        "id": "VtiMLjRSojDh"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-fold cross validation"
      ],
      "metadata": {
        "id": "GVjun6LziQnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_bert_model(model, dataset, batch_size=batchsize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CddFJpvqpvyX",
        "outputId": "f2aed6f6-d098-4533-c5e2-4f6116e8dce3"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU cuda\n",
            "------------fold no---------0----------------------\n",
            "[ 0  1  3  4  5  6  7  8  9 11 12 14 15 16 17 18 19 20 21 22 23 25 27 29]\n",
            "[ 2 10 13 24 26 28]\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "step: 0\n",
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 1.17\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------fold no---------1----------------------\n",
            "[ 0  1  2  3  4  6  7  8  9 10 12 13 14 15 18 19 20 21 23 24 25 26 28 29]\n",
            "[ 5 11 16 17 22 27]\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.97\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.90\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.92\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------fold no---------2----------------------\n",
            "[ 0  2  3  4  5  6  7  9 10 11 12 13 15 16 17 18 19 21 22 24 25 26 27 28]\n",
            "[ 1  8 14 20 23 29]\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.82\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  5.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.82\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  5.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------fold no---------3----------------------\n",
            "[ 0  1  2  3  5  8 10 11 12 13 14 15 16 17 20 21 22 23 24 25 26 27 28 29]\n",
            "[ 4  6  7  9 18 19]\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.76\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.73\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  5.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.74\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  5.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------fold no---------4----------------------\n",
            "[ 1  2  4  5  6  7  8  9 10 11 13 14 16 17 18 19 20 22 23 24 26 27 28 29]\n",
            "[ 0  3 12 15 21 25]\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.72\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  5.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.65\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  5.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.61\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  5.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ],
      "metadata": {
        "id": "Q4DIbEr6iTTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros(len(X))\n",
        "\n",
        "flat_logits = run_bert_model(model, dataset, batch_size=batchsize)\n",
        "y_pred = np.argmax(flat_logits, axis=1).flatten()\n",
        "\n",
        "df['BERT_label'] = y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOZjWvnYcuEi",
        "outputId": "4ad31879-d20f-4f6e-8123-0f76e4367bb1"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 30 test sentences...\n",
            "Using GPU cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 4/30 [00:00<00:05,  4.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROBERTA"
      ],
      "metadata": {
        "id": "PqE--Wf4yOR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainedBertModel = 'roberta'"
      ],
      "metadata": {
        "id": "FbmpPXXIyP64"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if trainedBertModel == 'bert':\n",
        "    used_bert_model = pretrained_models[trainedBertModel]\n",
        "    model = BertForSequenceClassification.from_pretrained(used_bert_model, num_labels = 3)\n",
        "    tokenizer = BertTokenizer.from_pretrained(used_bert_model, do_lower_case=True)\n",
        "elif trainedBertModel == 'roberta':\n",
        "    used_bert_model = pretrained_models[trainedBertModel]\n",
        "    model = RobertaForSequenceClassification.from_pretrained(used_bert_model, num_labels = 3)\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(used_bert_model, do_lower_case=True)\n",
        "\n",
        "input_ids, attention_masks, labels = prepare_dataset(X, Y, tokenizer, max_length=400)\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)"
      ],
      "metadata": {
        "id": "DsYnxFyLyd14"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-fold cross validation"
      ],
      "metadata": {
        "id": "V70JddqkygVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_bert_model(model, dataset, batch_size=batchsize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_bpJn7gypUT",
        "outputId": "5774e703-e2b2-43a6-d7d5-d02159ad3270"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU cuda\n",
            "------------fold no---------0----------------------\n",
            "[ 0  1  2  3  4  7  9 10 11 12 14 15 17 18 19 21 22 23 24 25 26 27 28 29]\n",
            "[ 5  6  8 13 16 20]\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "step: 0\n",
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 1.09\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 1.03\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------fold no---------1----------------------\n",
            "[ 0  1  3  5  6  7  8  9 10 13 14 15 16 17 18 19 20 21 22 23 24 27 28 29]\n",
            "[ 2  4 11 12 25 26]\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.96\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.97\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------fold no---------2----------------------\n",
            "[ 0  2  3  4  5  6  7  8 11 12 13 14 15 16 17 18 19 20 21 22 24 25 26 29]\n",
            "[ 1  9 10 23 27 28]\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.92\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.86\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.85\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------fold no---------3----------------------\n",
            "[ 0  1  2  3  4  5  6  8  9 10 11 12 13 15 16 18 20 23 24 25 26 27 28 29]\n",
            "[ 7 14 17 19 21 22]\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.78\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.78\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------fold no---------4----------------------\n",
            "[ 1  2  4  5  6  7  8  9 10 11 12 13 14 16 17 19 20 21 22 23 25 26 27 28]\n",
            "[ 0  3 15 18 24 29]\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.76\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "step: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1\n",
            "step: 2\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epoch took: 0:00:02\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:00<00:00,  6.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "LqzVqDNRysxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros(len(X))\n",
        "\n",
        "flat_logits = run_bert_model(model, dataset, batch_size=batchsize)\n",
        "y_pred = np.argmax(flat_logits, axis=1).flatten()\n",
        "\n",
        "df['ROBERTA_label'] = y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYBvH3PUyp5b",
        "outputId": "6264cc21-58cf-49e6-e824-14309e3fb70b"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 30 test sentences...\n",
            "Using GPU cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 4/30 [00:00<00:04,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Tt3LA5qfzR0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "4zrPhMWQyv6O",
        "outputId": "584f1c6c-a90d-4653-eb93-6180d1c533e1"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Tweet ID                                               Text  \\\n",
              "0   1242553623260868608  Are we still allowed to quote ancient Chinese ...   \n",
              "1   1246508137638580225  @mamacat2u @VBeltiz More power to you!  This C...   \n",
              "2   1233468243534372865  CNBC: WHO, Tedros reiterated that the virus co...   \n",
              "3   1243626072387747841  \"The heightened racism experienced by Asian co...   \n",
              "4   1225611530978217989  Coronavirus and Nepali in China: KP Oli has di...   \n",
              "5   1238081115182764032  #IamNotAVirus - heard of it @thetimes ? Why us...   \n",
              "6   1246282568414179329  @RepAdamSchiff You have proven over the past 3...   \n",
              "7   1232094383924707328  @realDonaldTrump Fact: Coronavirus will probab...   \n",
              "8   1238072679694864384  for the last fucking time.... CORONAVIRUS IS N...   \n",
              "9   1245152684820348929  @realDonaldTrump I think your campaign slogan ...   \n",
              "10  1245357525400051712  Fuck You, China. (Covid19) https://t.co/kNlwgY...   \n",
              "11  1245999294139772928  Anyone else do a sneaky fist pump when every r...   \n",
              "12  1230523783708061696  Well no shit. #coronavirus https://t.co/1OLHdC...   \n",
              "13  1237480830097862658  Hoping April 1st come around and all the news ...   \n",
              "14  1247596322657570823  @MarshaBlackburn @FreedomHK4ever Most disgusti...   \n",
              "15  1244721630351327232  I am tweeting to Support #IAmNotAVirus hashtag...   \n",
              "16  1230394065004453888  Another Wuhan hospital chief 'is hospitalised ...   \n",
              "17  1230515621315710977  @mattwander12 I barback on the weekends and I ...   \n",
              "18  1226914308270981128  Are you worried about the Coronavirus?  #coron...   \n",
              "19  1244262818595000325  I'm losing my mind 😵 quarantine is making me m...   \n",
              "20  1230292697694580736  The Gray Plague: What The Fuck Is Going On In ...   \n",
              "21  1244844999566446592  CCP China doing all types of shady &amp; evil ...   \n",
              "22  1244398148174438400  @realDonaldTrump so much for it being the Chin...   \n",
              "23  1235395447214288897  Husband, I am SORRY you got what the rest of u...   \n",
              "24  1234510147667365888  Coronavirus Election Puts the Democrats In a D...   \n",
              "25  1233532837179072517  Have you ever seen anyone praying for death to...   \n",
              "26  1245035261030838272                @ChangshaCity fucking #chinesevirus   \n",
              "27  1238017385761656832  fuck china  #WuhanVirus #COVID2019 #coronaviru...   \n",
              "28  1241506262216593409  Watch this and not cry...wtf America. Seriousl...   \n",
              "29  1245443836777648128             This is the truth #ChinaLiedPeopleDied   \n",
              "\n",
              "    label  BERT_label  ROBERTA_label  \n",
              "0       0           0              0  \n",
              "1       0           0              0  \n",
              "2       0           0              0  \n",
              "3       1           0              0  \n",
              "4       0           0              0  \n",
              "5       1           0              0  \n",
              "6       0           0              0  \n",
              "7       0           0              0  \n",
              "8       1           0              0  \n",
              "9       0           0              0  \n",
              "10      2           2              0  \n",
              "11      0           0              0  \n",
              "12      0           0              0  \n",
              "13      0           0              0  \n",
              "14      0           0              0  \n",
              "15      1           0              0  \n",
              "16      0           0              0  \n",
              "17      0           0              0  \n",
              "18      0           0              0  \n",
              "19      2           2              0  \n",
              "20      0           0              0  \n",
              "21      2           2              0  \n",
              "22      0           0              0  \n",
              "23      0           0              0  \n",
              "24      2           0              0  \n",
              "25      0           0              0  \n",
              "26      2           0              0  \n",
              "27      2           0              0  \n",
              "28      1           0              0  \n",
              "29      0           0              0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ecfd357e-e360-4d81-9fa7-63a4c11ae207\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet ID</th>\n",
              "      <th>Text</th>\n",
              "      <th>label</th>\n",
              "      <th>BERT_label</th>\n",
              "      <th>ROBERTA_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1242553623260868608</td>\n",
              "      <td>Are we still allowed to quote ancient Chinese ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1246508137638580225</td>\n",
              "      <td>@mamacat2u @VBeltiz More power to you!  This C...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1233468243534372865</td>\n",
              "      <td>CNBC: WHO, Tedros reiterated that the virus co...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1243626072387747841</td>\n",
              "      <td>\"The heightened racism experienced by Asian co...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1225611530978217989</td>\n",
              "      <td>Coronavirus and Nepali in China: KP Oli has di...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1238081115182764032</td>\n",
              "      <td>#IamNotAVirus - heard of it @thetimes ? Why us...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1246282568414179329</td>\n",
              "      <td>@RepAdamSchiff You have proven over the past 3...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1232094383924707328</td>\n",
              "      <td>@realDonaldTrump Fact: Coronavirus will probab...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1238072679694864384</td>\n",
              "      <td>for the last fucking time.... CORONAVIRUS IS N...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1245152684820348929</td>\n",
              "      <td>@realDonaldTrump I think your campaign slogan ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1245357525400051712</td>\n",
              "      <td>Fuck You, China. (Covid19) https://t.co/kNlwgY...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1245999294139772928</td>\n",
              "      <td>Anyone else do a sneaky fist pump when every r...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1230523783708061696</td>\n",
              "      <td>Well no shit. #coronavirus https://t.co/1OLHdC...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1237480830097862658</td>\n",
              "      <td>Hoping April 1st come around and all the news ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1247596322657570823</td>\n",
              "      <td>@MarshaBlackburn @FreedomHK4ever Most disgusti...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1244721630351327232</td>\n",
              "      <td>I am tweeting to Support #IAmNotAVirus hashtag...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1230394065004453888</td>\n",
              "      <td>Another Wuhan hospital chief 'is hospitalised ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1230515621315710977</td>\n",
              "      <td>@mattwander12 I barback on the weekends and I ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1226914308270981128</td>\n",
              "      <td>Are you worried about the Coronavirus?  #coron...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1244262818595000325</td>\n",
              "      <td>I'm losing my mind 😵 quarantine is making me m...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1230292697694580736</td>\n",
              "      <td>The Gray Plague: What The Fuck Is Going On In ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1244844999566446592</td>\n",
              "      <td>CCP China doing all types of shady &amp;amp; evil ...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1244398148174438400</td>\n",
              "      <td>@realDonaldTrump so much for it being the Chin...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1235395447214288897</td>\n",
              "      <td>Husband, I am SORRY you got what the rest of u...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1234510147667365888</td>\n",
              "      <td>Coronavirus Election Puts the Democrats In a D...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1233532837179072517</td>\n",
              "      <td>Have you ever seen anyone praying for death to...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1245035261030838272</td>\n",
              "      <td>@ChangshaCity fucking #chinesevirus</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1238017385761656832</td>\n",
              "      <td>fuck china  #WuhanVirus #COVID2019 #coronaviru...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1241506262216593409</td>\n",
              "      <td>Watch this and not cry...wtf America. Seriousl...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1245443836777648128</td>\n",
              "      <td>This is the truth #ChinaLiedPeopleDied</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecfd357e-e360-4d81-9fa7-63a4c11ae207')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ecfd357e-e360-4d81-9fa7-63a4c11ae207 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ecfd357e-e360-4d81-9fa7-63a4c11ae207');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT"
      ],
      "metadata": {
        "id": "qZz_Q_BrzZfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(df['label'], df['BERT_label'])"
      ],
      "metadata": {
        "id": "foDD1940yxnS"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1), index = [i for i in range(3)],\n",
        "                     columns = [i for i in range(3)])"
      ],
      "metadata": {
        "id": "Q87Vq_x7yzTW"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (3,3))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "YZq8u4qoy2Iv",
        "outputId": "702fa8bb-4953-4154-a2a9-5693a3b0417f"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(df['label'], df['BERT_label'], digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFYK-FXdy3qY",
        "outputId": "646f63c6-6ae3-43bd-9432-0883dade101c"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.704     1.000     0.826        19\n",
            "           1      0.000     0.000     0.000         5\n",
            "           2      1.000     0.500     0.667         6\n",
            "\n",
            "    accuracy                          0.733        30\n",
            "   macro avg      0.568     0.500     0.498        30\n",
            "weighted avg      0.646     0.733     0.657        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROBERTA"
      ],
      "metadata": {
        "id": "GYS1LMinzazG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(df['label'], df['ROBERTA_label'])"
      ],
      "metadata": {
        "id": "Nc4MdYAXzcnW"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1), index = [i for i in range(3)],\n",
        "                     columns = [i for i in range(3)])"
      ],
      "metadata": {
        "id": "WHuCyb11zeqA"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (3,3))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "lN6EauDRzfk6",
        "outputId": "7ad1801e-9858-484f-b69d-65b1e914752a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(df['label'], df['ROBERTA_label'], digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GELFLRPYzhzo",
        "outputId": "9c7b496d-07a8-4b2b-dab8-d5db4df093d1"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.633     1.000     0.776        19\n",
            "           1      0.000     0.000     0.000         5\n",
            "           2      0.000     0.000     0.000         6\n",
            "\n",
            "    accuracy                          0.633        30\n",
            "   macro avg      0.211     0.333     0.259        30\n",
            "weighted avg      0.401     0.633     0.491        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}