{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "psLAHX-ibM_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers~=2.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QarvIBQ2ao5X",
        "outputId": "64816dd4-0bbe-4a1d-b9c0-4751b1beb440"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers~=2.11.0 in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (2.25.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (23.0)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (0.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (2022.6.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (0.1.97)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (0.0.53)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers~=2.11.0) (1.22.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers~=2.11.0) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers~=2.11.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers~=2.11.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers~=2.11.0) (2022.12.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers~=2.11.0) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers~=2.11.0) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers~=2.11.0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjdiD93JbKhs",
        "outputId": "d5fa0bb1-0d26-4752-8d17-074d22ec0675"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.8/dist-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.22.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.26.82)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.5.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.82 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (1.29.82)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.82->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.82->boto3->pytorch_pretrained_bert) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULou06Z5bUGd",
        "outputId": "20d5e980-ad85-4388-c730-1573802c634b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Google Drive"
      ],
      "metadata": {
        "id": "MwswTFhymZtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH4pe4CLmZ4J",
        "outputId": "09cdd2de-dcc3-420c-e745-fb5bbbec82e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive\")"
      ],
      "metadata": {
        "id": "DepiXFaZmj95"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "J4pchGlNoovq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_data_path = './data/capstone/CLAWS/covidhate/annotated_tweets_w_text.csv'\n",
        "part_data_path = './data/capstone/CLAWS/covidhate/part_labeled_tweet.csv'\n",
        "#data_path = './data/part_labeled_tweet.csv'\n",
        "data_path = all_data_path"
      ],
      "metadata": {
        "id": "DbhPwu1smlmS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "none_save_path = './Colab Notebooks/ucla_capstone/saved_model/none'\n",
        "bert_save_path = './Colab Notebooks/ucla_capstone/saved_model/bert'\n",
        "roberta_save_path = './Colab Notebooks/ucla_capstone/saved_model/roberta'"
      ],
      "metadata": {
        "id": "NpZjq7ztpKWH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batchsize = 8"
      ],
      "metadata": {
        "id": "vGo1KXEEo_W6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_num = 0"
      ],
      "metadata": {
        "id": "1Ypeg_iP1GIX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_stats_dict = {}"
      ],
      "metadata": {
        "id": "fTEgg2Ri4_oO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "4_XmfbDvbbRQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4xFudb4HZv9v"
      },
      "outputs": [],
      "source": [
        "# Basic and System\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time, datetime, random, glob, os, sys, joblib, argparse, json\n",
        "# Torch\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, Subset\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "# Pretrained Models\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from tqdm import tqdm \n",
        "# Scheduler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "# OPtimizer\n",
        "from transformers import AdamW\n",
        "# Kfold Cross Validation\n",
        "from sklearn.model_selection import KFold\n",
        "# Evaluation\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "#from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Variables\n",
        "USING_GPU = False\n",
        "DEVICE = None"
      ],
      "metadata": {
        "id": "pOfyaHj7av2_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained models dictionary\n",
        "# Try in the future: Albert\n",
        "pretrained_models = {'bert': 'bert-base-uncased', 'roberta': 'roberta-base'}"
      ],
      "metadata": {
        "id": "gTNRo7cnb-K3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(seed_num)\n",
        "np.random.seed(seed_num)\n",
        "torch.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed_all(seed_num)"
      ],
      "metadata": {
        "id": "jY-AhH3N1IYB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format time display\n",
        "def format_time(seconds):\n",
        "    seconds_round = int(round((seconds)))\n",
        "    return str(datetime.timedelta(seconds=seconds_round)) # hh:mm:ss"
      ],
      "metadata": {
        "id": "f5pCIOgyX8te"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for pytorch dataloader\n",
        "def prepare_dataset(sentences, labels, tokenizer, max_length=100):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        # print(sent)\n",
        "        try:\n",
        "            encoded_dict = tokenizer.encode_plus(\n",
        "                                sent,\n",
        "                                add_special_tokens = True,\n",
        "                                max_length = max_length,\n",
        "                                truncation=True,\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,\n",
        "                                return_tensors = 'pt'\n",
        "                           )\n",
        "        except:\n",
        "            print(\"some tweet sent is not correct\")\n",
        "            print(sent)\n",
        "            exit(0)\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    return input_ids, attention_masks, labels"
      ],
      "metadata": {
        "id": "K1oI7oXzcyRT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(fold, model, device, train_loader, optimizer, scheduler, epoch):\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        #print('step: '+str(step))\n",
        "        model.zero_grad()\n",
        "        \n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n",
        "        \n",
        "        b_input_ids = batch[0].to(DEVICE)\n",
        "        b_input_mask = batch[1].to(DEVICE)\n",
        "        b_labels = batch[2].unsqueeze(0).to(DEVICE)\n",
        "        #print(b_input_ids.shape)\n",
        "        #print(b_input_mask.shape)\n",
        "        #print(b_labels.shape)\n",
        "        # https://stackoverflow.com/questions/70548318/bertforsequenceclassification-target-size-torch-size1-16-must-be-the-same\n",
        "        #b_labels = torch.nn.functional.one_hot(b_labels.to(torch.int64), 3)\n",
        "        #print(type(b_labels))\n",
        "        #print(b_labels.shape)\n",
        "        #loss, logits, hidden_states = model(b_input_ids,\n",
        "        loss, logits = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "        #print('run more')\n",
        "        total_train_loss += loss.item()\n",
        "        #loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        del loss, logits\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    return avg_train_loss, training_time"
      ],
      "metadata": {
        "id": "042JRUkecyUC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "1bAzPi6vw6vo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(fold, model, device, val_loader, val_data_len):\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    # Measure how long the validation epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    \n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    #predictions, true_labels = [], []\n",
        "    for batch in tqdm(val_loader, total=val_data_len):\n",
        "        # b_labels are true labels\n",
        "        b_input_ids = batch[0].to(DEVICE)\n",
        "        b_input_mask = batch[1].to(DEVICE)\n",
        "        b_labels = batch[2].unsqueeze(0).to(DEVICE)\n",
        "        #b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            loss, logits = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            #b_proba = outputs[0]\n",
        "\n",
        "            #print(b_proba)\n",
        "            #print(b_proba.item())\n",
        "            # Accumulate the validation loss.\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            #proba = b_proba.detach().cpu().numpy()\n",
        "            #label_ids = batch[2].numpy()\n",
        "\n",
        "            #predictions.append(proba)\n",
        "            #true_labels.append(label_ids)\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            # Calculate the accuracy for this batch of test sentences, and\n",
        "            # accumulate it over all batches.\n",
        "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
        "    print(\"\")\n",
        "    print(\"  Average validation  accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "    \n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(val_loader)\n",
        "\n",
        "    val_time = format_time(time.time() - t0)\n",
        "    print(\"  Average validation loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation epoch took: {:}\".format(val_time))\n",
        "\n",
        "    return avg_val_accuracy, avg_val_loss, val_time"
      ],
      "metadata": {
        "id": "JGSJos5QcyWy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper function for fine-tuning\n",
        "# batch size, epochs, and learning rate are from CLAWS paper\n",
        "def train_bert_model(model, dataset, train_stats, batch_size, epochs=2, learning_rate=1e-5, epsilon=1e-8):\n",
        "\n",
        "    if USING_GPU:\n",
        "        print(\"Using GPU\", DEVICE)\n",
        "        model.cuda(DEVICE)\n",
        "\n",
        "    # Measure how long the training takes.\n",
        "    total_t0 = time.time()\n",
        "\n",
        "    # prepare cross validation\n",
        "    n_fold = 5\n",
        "    kfold = KFold(n_splits=n_fold, shuffle=True)\n",
        "\n",
        "    # for each fold\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print()\n",
        "        print('------------fold no---------{}----------------------'.format(fold + 1))\n",
        "        #print(train_idx)\n",
        "        #print(val_idx)\n",
        "        train_tensor = Subset(dataset, train_idx)\n",
        "        val_tensor = Subset(dataset, val_idx)\n",
        "        val_data_len = len(val_idx)\n",
        "\n",
        "        trainloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=RandomSampler(train_tensor))\n",
        "        valloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=SequentialSampler(val_tensor))\n",
        "        \n",
        "        total_steps = len(trainloader) * epochs\n",
        "        optimizer = AdamW(model.parameters(),\n",
        "                          lr=learning_rate,\n",
        "                          eps=epsilon\n",
        "                          )\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                               num_warmup_steps=0,  # Default value in run_glue.py\n",
        "                               num_training_steps=total_steps)\n",
        "        \n",
        "        for epoch_i in range(0, epochs):\n",
        "            print()\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            avg_train_loss, training_time = train(fold, model, DEVICE, trainloader, optimizer, scheduler, epochs)\n",
        "            avg_val_accuracy, avg_val_loss, validation_time = validation(fold, model, DEVICE, valloader, val_data_len)\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'fold': fold + 1,\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "    total_time = format_time(time.time() - total_t0)\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "    \n",
        "    print(\"Total training took: {:}\".format(total_time))"
      ],
      "metadata": {
        "id": "4jE2K2IpcyZk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict data\n",
        "def run_bert_model(model, test_dataset, batch_size):    \n",
        "    print('Predicting labels for {:,} test sentences...'.format(len(test_dataset)))\n",
        "    \n",
        "    if USING_GPU:\n",
        "        print(\"Using GPU\", DEVICE)\n",
        "        model.cuda(DEVICE)\n",
        "\n",
        "    model.eval()\n",
        "    predictions , true_labels = [], []\n",
        "    prediction_sampler = SequentialSampler(test_dataset)\n",
        "    prediction_dataloader = DataLoader(test_dataset, sampler=prediction_sampler, batch_size=batch_size)\n",
        "    for batch in tqdm(prediction_dataloader, total=len(test_dataset)):\n",
        "        \n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch[0].to(DEVICE), token_type_ids=None,\n",
        "                                  attention_mask=batch[1].to(DEVICE))\n",
        "            b_proba = outputs[0]\n",
        "\n",
        "            proba = b_proba.detach().cpu().numpy()\n",
        "            label_ids = batch[2].numpy()\n",
        "\n",
        "            predictions.append(proba)\n",
        "            true_labels.append(label_ids)\n",
        "\n",
        "    print('    DONE.')\n",
        "   \n",
        "    flat_predictions = np.concatenate(predictions, axis=0)\n",
        "    return flat_predictions"
      ],
      "metadata": {
        "id": "4SPuk_csh0tC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    USING_GPU = True\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    USING_GPU = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaAtAq-hcycH",
        "outputId": "1f77b0ac-5057-4cc7-e1d7-722c2023e585"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(data_path)\n",
        "df = df[df['Text'].notna()]\n",
        "X = df.Text.values # x\n",
        "Y = list(df['label']) # y_true"
      ],
      "metadata": {
        "id": "mPy1HwuQoWf2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No Pretrained"
      ],
      "metadata": {
        "id": "j1hCOQ1oBMNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainedBertModel = 'none'"
      ],
      "metadata": {
        "id": "DfamrnEBBfa0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if trainedBertModel == 'none':\n",
        "    config = BertConfig() # You can also change the architecture using this config class\n",
        "    config.num_labels = 3\n",
        "    model = BertForSequenceClassification(config=config)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "elif trainedBertModel == 'bert':\n",
        "    used_bert_model = pretrained_models[trainedBertModel]\n",
        "    model = BertForSequenceClassification.from_pretrained(used_bert_model, num_labels = 3)\n",
        "    tokenizer = BertTokenizer.from_pretrained(used_bert_model, do_lower_case=True)\n",
        "elif trainedBertModel == 'roberta':\n",
        "    used_bert_model = pretrained_models[trainedBertModel]\n",
        "    model = RobertaForSequenceClassification.from_pretrained(used_bert_model, num_labels = 3)\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(used_bert_model, do_lower_case=True)\n",
        "\n",
        "\n",
        "input_ids, attention_masks, labels = prepare_dataset(X, Y, tokenizer, max_length=400)\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)"
      ],
      "metadata": {
        "id": "tLGK5FWgBLsp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-fold cross validation"
      ],
      "metadata": {
        "id": "h3ewdlmKDQbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store training states such as training and validation loss, \n",
        "# validation accuracy, and timings for analysis\n",
        "training_stats = []\n",
        "train_bert_model(model, dataset, training_stats, batch_size=batchsize)\n",
        "training_stats_dict[trainedBertModel] = training_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2oUbsAeB5_L",
        "outputId": "7e25d0df-d651-4be4-9d06-e46c9a1033ba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU cuda\n",
            "\n",
            "------------fold no---------1----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:22.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:43.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:05.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:27.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:49.\n",
            "\n",
            "  Average training loss: 0.98\n",
            "  Training epoch took: 0:02:05\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:11<01:18,  5.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.58\n",
            "  Average validation loss: 0.94\n",
            "  Validation epoch took: 0:00:11\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:45.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:08.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:31.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:54.\n",
            "\n",
            "  Average training loss: 0.85\n",
            "  Training epoch took: 0:02:11\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:11<01:22,  4.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.64\n",
            "  Average validation loss: 0.79\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "------------fold no---------2----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.73\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.69\n",
            "  Average validation loss: 0.99\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.83\n",
            "  Average validation loss: 0.46\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "------------fold no---------3----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.85\n",
            "  Average validation loss: 0.37\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0:02:16\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.86\n",
            "  Average validation loss: 0.34\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "------------fold no---------4----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.84\n",
            "  Average validation loss: 0.52\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0:02:16\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.89\n",
            "  Average validation loss: 0.28\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "------------fold no---------5----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.85\n",
            "  Average validation loss: 0.51\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.93\n",
            "  Average validation loss: 0.21\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "Training complete!\n",
            "Total training took: 0:24:20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ],
      "metadata": {
        "id": "EtIUAZ8RDUL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros(len(X))\n",
        "\n",
        "flat_logits = run_bert_model(model, dataset, batch_size=batchsize)\n",
        "y_pred = np.argmax(flat_logits, axis=1).flatten()\n",
        "\n",
        "df['NONE_label'] = y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXn5a24ADUdn",
        "outputId": "e4d0015e-5197-41c9-d37c-5fdfb121bbf8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 2,290 test sentences...\n",
            "Using GPU cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 287/2290 [01:01<07:06,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model"
      ],
      "metadata": {
        "id": "grJGwNmgELj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(none_save_path)"
      ],
      "metadata": {
        "id": "gAy1m1tZELtX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "LzKnryqSyJiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainedBertModel = 'bert'"
      ],
      "metadata": {
        "id": "lJf2Pbqg-LFM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if trainedBertModel == 'bert':\n",
        "    used_bert_model = pretrained_models[trainedBertModel]\n",
        "    model = BertForSequenceClassification.from_pretrained(used_bert_model, num_labels = 3)\n",
        "    tokenizer = BertTokenizer.from_pretrained(used_bert_model, do_lower_case=True)\n",
        "elif trainedBertModel == 'roberta':\n",
        "    used_bert_model = pretrained_models[trainedBertModel]\n",
        "    model = RobertaForSequenceClassification.from_pretrained(used_bert_model, num_labels = 3)\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(used_bert_model, do_lower_case=True)\n",
        "\n",
        "input_ids, attention_masks, labels = prepare_dataset(X, Y, tokenizer, max_length=400)\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)"
      ],
      "metadata": {
        "id": "VtiMLjRSojDh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-fold cross validation"
      ],
      "metadata": {
        "id": "GVjun6LziQnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store training states such as training and validation loss, \n",
        "# validation accuracy, and timings for analysis\n",
        "training_stats = []\n",
        "train_bert_model(model, dataset, training_stats, batch_size=batchsize)\n",
        "training_stats_dict[trainedBertModel] = training_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CddFJpvqpvyX",
        "outputId": "2a96ef79-1fac-4e4a-c564-3e290c426720"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU cuda\n",
            "\n",
            "------------fold no---------1----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.77\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.82\n",
            "  Average validation loss: 0.50\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0:02:16\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.85\n",
            "  Average validation loss: 0.41\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "------------fold no---------2----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.87\n",
            "  Average validation loss: 0.32\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.91\n",
            "  Average validation loss: 0.23\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "------------fold no---------3----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.96\n",
            "  Average validation loss: 0.14\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epoch took: 0:02:16\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.97\n",
            "  Average validation loss: 0.10\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "------------fold no---------4----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.98\n",
            "  Average validation loss: 0.07\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epoch took: 0:02:16\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.98\n",
            "  Average validation loss: 0.06\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "------------fold no---------5----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epoch took: 0:02:15\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.99\n",
            "  Average validation loss: 0.04\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:58.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epoch took: 0:02:16\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:12<01:24,  4.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.99\n",
            "  Average validation loss: 0.02\n",
            "  Validation epoch took: 0:00:12\n",
            "\n",
            "Training complete!\n",
            "Total training took: 0:24:37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ],
      "metadata": {
        "id": "Q4DIbEr6iTTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros(len(X))\n",
        "\n",
        "flat_logits = run_bert_model(model, dataset, batch_size=batchsize)\n",
        "y_pred = np.argmax(flat_logits, axis=1).flatten()\n",
        "\n",
        "df['BERT_label'] = y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOZjWvnYcuEi",
        "outputId": "fdcd5a06-28b3-46f4-9228-9c958295842e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 2,290 test sentences...\n",
            "Using GPU cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 287/2290 [01:01<07:07,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model"
      ],
      "metadata": {
        "id": "DGi-MJ2uqTvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(bert_save_path)"
      ],
      "metadata": {
        "id": "340xdKt3p9ss"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROBERTA"
      ],
      "metadata": {
        "id": "PqE--Wf4yOR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainedBertModel = 'roberta'"
      ],
      "metadata": {
        "id": "FbmpPXXIyP64"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if trainedBertModel == 'bert':\n",
        "    used_bert_model = pretrained_models[trainedBertModel]\n",
        "    model = BertForSequenceClassification.from_pretrained(used_bert_model, num_labels = 3)\n",
        "    tokenizer = BertTokenizer.from_pretrained(used_bert_model, do_lower_case=True)\n",
        "elif trainedBertModel == 'roberta':\n",
        "    used_bert_model = pretrained_models[trainedBertModel]\n",
        "    model = RobertaForSequenceClassification.from_pretrained(used_bert_model, num_labels = 3)\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(used_bert_model, do_lower_case=True)\n",
        "\n",
        "input_ids, attention_masks, labels = prepare_dataset(X, Y, tokenizer, max_length=400)\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)"
      ],
      "metadata": {
        "id": "DsYnxFyLyd14"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-fold cross validation"
      ],
      "metadata": {
        "id": "V70JddqkygVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store training states such as training and validation loss, \n",
        "# validation accuracy, and timings for analysis\n",
        "training_stats = []\n",
        "train_bert_model(model, dataset, training_stats, batch_size=batchsize)\n",
        "training_stats_dict[trainedBertModel] = training_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_bpJn7gypUT",
        "outputId": "f1b7d0e7-8a11-42a1-8ec9-16f0a16e81f6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU cuda\n",
            "\n",
            "------------fold no---------1----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:48.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss: 0.72\n",
            "  Training epoch took: 0:02:17\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:10<01:14,  5.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.81\n",
            "  Average validation loss: 0.46\n",
            "  Validation epoch took: 0:00:11\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:48.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0:02:16\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:10<01:14,  5.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.87\n",
            "  Average validation loss: 0.38\n",
            "  Validation epoch took: 0:00:11\n",
            "\n",
            "------------fold no---------2----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:48.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0:02:16\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:10<01:14,  5.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.89\n",
            "  Average validation loss: 0.36\n",
            "  Validation epoch took: 0:00:11\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:48.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:12.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:36.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epoch took: 0:02:17\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:10<01:14,  5.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.92\n",
            "  Average validation loss: 0.28\n",
            "  Validation epoch took: 0:00:11\n",
            "\n",
            "------------fold no---------3----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:48.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:12.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:36.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0:02:17\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:10<01:14,  5.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.96\n",
            "  Average validation loss: 0.17\n",
            "  Validation epoch took: 0:00:11\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:48.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:12.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epoch took: 0:02:17\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:10<01:14,  5.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.95\n",
            "  Average validation loss: 0.16\n",
            "  Validation epoch took: 0:00:11\n",
            "\n",
            "------------fold no---------4----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:48.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:12.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epoch took: 0:02:17\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:10<01:14,  5.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.98\n",
            "  Average validation loss: 0.10\n",
            "  Validation epoch took: 0:00:11\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:48.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:12.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epoch took: 0:02:17\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:10<01:14,  5.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.98\n",
            "  Average validation loss: 0.10\n",
            "  Validation epoch took: 0:00:11\n",
            "\n",
            "------------fold no---------5----------------------\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:48.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:12.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epoch took: 0:02:17\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:10<01:14,  5.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.98\n",
            "  Average validation loss: 0.12\n",
            "  Validation epoch took: 0:00:11\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    229.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    229.    Elapsed: 0:00:48.\n",
            "  Batch   120  of    229.    Elapsed: 0:01:12.\n",
            "  Batch   160  of    229.    Elapsed: 0:01:35.\n",
            "  Batch   200  of    229.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epoch took: 0:02:17\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 58/458 [00:10<01:14,  5.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average validation  accuracy: 0.99\n",
            "  Average validation loss: 0.06\n",
            "  Validation epoch took: 0:00:11\n",
            "\n",
            "Training complete!\n",
            "Total training took: 0:24:33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ],
      "metadata": {
        "id": "LqzVqDNRysxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros(len(X))\n",
        "\n",
        "flat_logits = run_bert_model(model, dataset, batch_size=batchsize)\n",
        "y_pred = np.argmax(flat_logits, axis=1).flatten()\n",
        "\n",
        "df['ROBERTA_label'] = y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYBvH3PUyp5b",
        "outputId": "a7b67013-7125-4b9f-edde-2767b1f59430"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 2,290 test sentences...\n",
            "Using GPU cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 287/2290 [00:53<06:13,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model"
      ],
      "metadata": {
        "id": "rH3W2zDRqRIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(roberta_save_path)"
      ],
      "metadata": {
        "id": "EI_by6-SqM2M"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Training Stats"
      ],
      "metadata": {
        "id": "43mlNwT57_Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pkl_path = './Colab Notebooks/ucla_capstone/saved_model/training_stats_dict.pkl'\n",
        "# save dictionary to training_stats_dict.pkl file\n",
        "with open(pkl_path, 'wb') as fp:\n",
        "    pickle.dump(training_stats_dict, fp)\n",
        "    print('dictionary saved successfully to file')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huhgx7608Db6",
        "outputId": "5b04d5e1-58d0-4fe7-8081-9f0aee6edaa5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionary saved successfully to file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If needed to load\n",
        "'''\n",
        "stats_load = {}\n",
        "with open(pkl_path, 'rb') as handle:\n",
        "    stats_load = pickle.load(handle)\n",
        "print(stats_load)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L2fDx_MMAg8a",
        "outputId": "9a7c8438-c39b-4301-b571-df5b831a1255"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nstats_load = {}\\nwith open(pkl_path, 'rb') as handle:\\n    stats_load = pickle.load(handle)\\nprint(stats_load)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Tt3LA5qfzR0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "4zrPhMWQyv6O",
        "outputId": "2f96980f-5a1b-4a9e-b163-9544966283f2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Tweet ID                                               Text  \\\n",
              "0   1242553623260868608  Are we still allowed to quote ancient Chinese ...   \n",
              "1   1246508137638580225  @mamacat2u @VBeltiz More power to you!  This C...   \n",
              "2   1233468243534372865  CNBC: WHO, Tedros reiterated that the virus co...   \n",
              "3   1243626072387747841  \"The heightened racism experienced by Asian co...   \n",
              "4   1225611530978217989  Coronavirus and Nepali in China: KP Oli has di...   \n",
              "5   1238081115182764032  #IamNotAVirus - heard of it @thetimes ? Why us...   \n",
              "6   1246282568414179329  @RepAdamSchiff You have proven over the past 3...   \n",
              "7   1232094383924707328  @realDonaldTrump Fact: Coronavirus will probab...   \n",
              "8   1238072679694864384  for the last fucking time.... CORONAVIRUS IS N...   \n",
              "9   1245152684820348929  @realDonaldTrump I think your campaign slogan ...   \n",
              "10  1245357525400051712  Fuck You, China. (Covid19) https://t.co/kNlwgY...   \n",
              "11  1245999294139772928  Anyone else do a sneaky fist pump when every r...   \n",
              "12  1230523783708061696  Well no shit. #coronavirus https://t.co/1OLHdC...   \n",
              "13  1237480830097862658  Hoping April 1st come around and all the news ...   \n",
              "14  1247596322657570823  @MarshaBlackburn @FreedomHK4ever Most disgusti...   \n",
              "15  1244721630351327232  I am tweeting to Support #IAmNotAVirus hashtag...   \n",
              "16  1230394065004453888  Another Wuhan hospital chief 'is hospitalised ...   \n",
              "17  1230515621315710977  @mattwander12 I barback on the weekends and I ...   \n",
              "18  1226914308270981128  Are you worried about the Coronavirus?  #coron...   \n",
              "19  1244262818595000325  I'm losing my mind 😵 quarantine is making me m...   \n",
              "20  1230292697694580736  The Gray Plague: What The Fuck Is Going On In ...   \n",
              "21  1244844999566446592  CCP China doing all types of shady &amp; evil ...   \n",
              "22  1244398148174438400  @realDonaldTrump so much for it being the Chin...   \n",
              "23  1235395447214288897  Husband, I am SORRY you got what the rest of u...   \n",
              "24  1234510147667365888  Coronavirus Election Puts the Democrats In a D...   \n",
              "25  1233532837179072517  Have you ever seen anyone praying for death to...   \n",
              "26  1245035261030838272                @ChangshaCity fucking #chinesevirus   \n",
              "27  1238017385761656832  fuck china  #WuhanVirus #COVID2019 #coronaviru...   \n",
              "28  1241506262216593409  Watch this and not cry...wtf America. Seriousl...   \n",
              "29  1245443836777648128             This is the truth #ChinaLiedPeopleDied   \n",
              "\n",
              "    label  NONE_label  BERT_label  ROBERTA_label  \n",
              "0       0           1           0              0  \n",
              "1       0           0           0              0  \n",
              "2       0           0           0              0  \n",
              "3       1           1           1              1  \n",
              "4       0           0           0              0  \n",
              "5       1           1           1              1  \n",
              "6       0           0           0              0  \n",
              "7       0           0           0              0  \n",
              "8       1           1           1              1  \n",
              "9       0           0           0              0  \n",
              "10      2           2           2              2  \n",
              "11      0           0           0              0  \n",
              "12      0           0           0              0  \n",
              "13      0           0           0              0  \n",
              "14      0           2           0              0  \n",
              "15      1           1           1              1  \n",
              "16      0           0           0              0  \n",
              "17      0           0           0              0  \n",
              "18      0           0           0              0  \n",
              "19      2           2           2              2  \n",
              "20      0           0           0              0  \n",
              "21      2           2           2              2  \n",
              "22      0           0           0              0  \n",
              "23      0           0           0              0  \n",
              "24      2           0           2              0  \n",
              "25      0           0           0              0  \n",
              "26      2           2           2              2  \n",
              "27      2           2           2              2  \n",
              "28      1           1           1              1  \n",
              "29      0           2           0              0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5498f6f-ac57-4cc1-9e34-a27c6be013a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet ID</th>\n",
              "      <th>Text</th>\n",
              "      <th>label</th>\n",
              "      <th>NONE_label</th>\n",
              "      <th>BERT_label</th>\n",
              "      <th>ROBERTA_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1242553623260868608</td>\n",
              "      <td>Are we still allowed to quote ancient Chinese ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1246508137638580225</td>\n",
              "      <td>@mamacat2u @VBeltiz More power to you!  This C...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1233468243534372865</td>\n",
              "      <td>CNBC: WHO, Tedros reiterated that the virus co...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1243626072387747841</td>\n",
              "      <td>\"The heightened racism experienced by Asian co...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1225611530978217989</td>\n",
              "      <td>Coronavirus and Nepali in China: KP Oli has di...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1238081115182764032</td>\n",
              "      <td>#IamNotAVirus - heard of it @thetimes ? Why us...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1246282568414179329</td>\n",
              "      <td>@RepAdamSchiff You have proven over the past 3...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1232094383924707328</td>\n",
              "      <td>@realDonaldTrump Fact: Coronavirus will probab...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1238072679694864384</td>\n",
              "      <td>for the last fucking time.... CORONAVIRUS IS N...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1245152684820348929</td>\n",
              "      <td>@realDonaldTrump I think your campaign slogan ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1245357525400051712</td>\n",
              "      <td>Fuck You, China. (Covid19) https://t.co/kNlwgY...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1245999294139772928</td>\n",
              "      <td>Anyone else do a sneaky fist pump when every r...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1230523783708061696</td>\n",
              "      <td>Well no shit. #coronavirus https://t.co/1OLHdC...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1237480830097862658</td>\n",
              "      <td>Hoping April 1st come around and all the news ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1247596322657570823</td>\n",
              "      <td>@MarshaBlackburn @FreedomHK4ever Most disgusti...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1244721630351327232</td>\n",
              "      <td>I am tweeting to Support #IAmNotAVirus hashtag...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1230394065004453888</td>\n",
              "      <td>Another Wuhan hospital chief 'is hospitalised ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1230515621315710977</td>\n",
              "      <td>@mattwander12 I barback on the weekends and I ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1226914308270981128</td>\n",
              "      <td>Are you worried about the Coronavirus?  #coron...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1244262818595000325</td>\n",
              "      <td>I'm losing my mind 😵 quarantine is making me m...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1230292697694580736</td>\n",
              "      <td>The Gray Plague: What The Fuck Is Going On In ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1244844999566446592</td>\n",
              "      <td>CCP China doing all types of shady &amp;amp; evil ...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1244398148174438400</td>\n",
              "      <td>@realDonaldTrump so much for it being the Chin...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1235395447214288897</td>\n",
              "      <td>Husband, I am SORRY you got what the rest of u...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1234510147667365888</td>\n",
              "      <td>Coronavirus Election Puts the Democrats In a D...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1233532837179072517</td>\n",
              "      <td>Have you ever seen anyone praying for death to...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1245035261030838272</td>\n",
              "      <td>@ChangshaCity fucking #chinesevirus</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1238017385761656832</td>\n",
              "      <td>fuck china  #WuhanVirus #COVID2019 #coronaviru...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1241506262216593409</td>\n",
              "      <td>Watch this and not cry...wtf America. Seriousl...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1245443836777648128</td>\n",
              "      <td>This is the truth #ChinaLiedPeopleDied</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5498f6f-ac57-4cc1-9e34-a27c6be013a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f5498f6f-ac57-4cc1-9e34-a27c6be013a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f5498f6f-ac57-4cc1-9e34-a27c6be013a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No Pretrained"
      ],
      "metadata": {
        "id": "r8nnNqDjDqKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(df['label'], df['NONE_label'])"
      ],
      "metadata": {
        "id": "knRxH8orDpob"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1), index = [i for i in range(3)],\n",
        "                     columns = [i for i in range(3)])"
      ],
      "metadata": {
        "id": "vaB_j3yND1Cd"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (3,3))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Vh1FzyKuD2E0",
        "outputId": "7f5e3ca7-1dd4-4a46-aa41-80bcb2236d8b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(df['label'], df['NONE_label'], digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAvEN7mnD3OX",
        "outputId": "812ccedc-691c-437f-c7ca-692f72f813cb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.917     0.924     0.921      1344\n",
            "           1      0.928     0.894     0.910       517\n",
            "           2      0.826     0.844     0.835       429\n",
            "\n",
            "    accuracy                          0.902      2290\n",
            "   macro avg      0.890     0.887     0.889      2290\n",
            "weighted avg      0.903     0.902     0.902      2290\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT"
      ],
      "metadata": {
        "id": "qZz_Q_BrzZfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(df['label'], df['BERT_label'])"
      ],
      "metadata": {
        "id": "foDD1940yxnS"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1), index = [i for i in range(3)],\n",
        "                     columns = [i for i in range(3)])"
      ],
      "metadata": {
        "id": "Q87Vq_x7yzTW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (3,3))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "YZq8u4qoy2Iv",
        "outputId": "fe487838-91cb-4856-c582-9a629f451bfe"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMwAAADCCAYAAAAFKC2CAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAc2UlEQVR4nO3dd3hUVfrA8e+bIdFVihJaCk0BaQoCovxkhSgiLRQFRIrigrgolgVUQBdBEbCgIgIWemClSwkoAkuRpoQOofckdBKKCCST8/tjYjYTYO4MmTg3u+/H5zxPZu65575e5p1z7pmZe8QYg1LKO0GBDkCpvEQTRikfaMIo5QNNGKV8oAmjlA80YZTyQb7cPkDq6QN5bt76jlKPBjoEn4QE5fo/o98lX9wnnranntxr+boJLlbeYxu5Ie+dafW/waQHOoLr0oRRtmScaYEO4bo0YZQ9pWsPo5T3nKmBjuC6NGGUPemQTCnvGb3oV8oH2sMo5YN0Z6AjuC5NGGVP2sMo5QO9hlHKe0anlZXygQ7JlPKBDsmU8oEOyZTygX6XTCkf6DWMUj7QHkYp7+m0slK+0CGZUj7QaWWlfGDTHsaWt1latS6OZu260rjt3xgTM/2a7UnHT9Dl1T60erY7nXu8yfGTpzK3DRs5lhYdXiS6fTcGfzaa3LzZ+uOP12PT5qVs3bacXr26X7M9JCSEiZO+ZOu25SxfMYdSpSIBePTRuqxaPZ9ff/2RVavnU69eHQDy57+dtesWZpbDRzby0Uf9/RbvYw0e4deNP7Fhy1Je7/nideMdO3E4G7YsZfGymZQsFeG2PTIyjKPHt9Dj1S4ARESEMW/hZNbG/cia9T/w4kvP+S1W0tOtSwDYLmGcTieDho1k9LD3mTflaxYuWc7+g4fd6nzy5RiaN3qM7yeNpvvz7fn8qwkAbNoWz6Zt8cyeNIo5MaPZsXMP6zdty5U4g4KC+PSz92jVsjM1azxOmzbNqVixnFud5zq3JSXlHPfdW58vR4zl/UF9ADhzJpnWrbtQu3Yjur3QizFjPwPg4sXfqPNQk8xy9Ggic+f+6Ld4P/50AG2e7MJDtRrxVJtm3JMt3k7PteFcyjlqVnuM0SPHM+D9N922Dxr6NksWr8x8nJaWxjt9h1CnViMaRrWm6wsdr2nzpjnTrEsA2C5htu3cQ6nIcEpGhBEcHEzjx+rx75/XudXZf/AItWtWB6B2jWos+3ktACLC1atXSU1L42pqKqlpTkIL35ErcdaqVZ0D+w9z6NBRUlNTmTlzPs2aNXSr06xpQ6ZMngXA998vpH79/wNgy5YdHD92EoD4+D3ceuuthISEuO1brlxZihYNZfXqX/0Sb81a1Thw4DCHM+KdPXMBTZo2cKvTuGkDvpvyPQBzv/+RevXrZG5r0qwBRw4dZdfOvZnPnThxiq1bdgCuZN+zez9hYcX9Eq/2MF46eeo0JYoVzXxcvFgRTp4641bnnvJ3sWTFagCWrFjDb5d+J+XceapXrcQDNe4jqnkHopp34OEHa3B3mVK5Emd4eHESEpMyHycmHiMsvPgN6zidTs6fv0Bo6J1udVq2bMyWzdu5evWq2/Ot20Qza2as3+INCy9OYsKxzMdJicevG+8fdZxOJ+fPXaRw6J3cfvttvPaPF/lwyIgbtl+yVAT3VavMhrgt/gk4r/YwIlJRRN4SkS8yylsiUunPCO5Ger/clbhN22jd+WXiNm+jeNFQgoKCOJKQxIFDR1n6fQz/njOZXzdsYcPm7YEM1aNKlcrz/qA+vPJKv2u2tW4dzfQZ8wIQ1bXe6vcqo0eO57ffLl13++2338akKSPp+9YgLly46J+D+iFhRKSRiOwWkX0i0uc620uJyDIR2SQiW0WkiVWbHmfJROQt4BlgKvDH2CAS+E5Ephpjht5gv25AN4BRwwbR9dlnrOLIVKxoEbeL+BMnT1OsaGi2OqEMH/JPAC5d+p0ly1dRsEB+Zs77kWpVKnLbbX8BoO5DtdiyYyc1q1f1+vjeSko6QWREeObjiIgwjiWduG6dpMTjOBwOChYswJkzyQCER5Tgu6lf80LXnhw8eMRtv3vvrUS+fA42b/Jfsh9LOkFEZFjm4/CIEteNNyIyjKSkjHgL5efsmWRqPVCNFi0bMfD9NylUqCDp6elcuXKVb7+OIV++fEycMpIZ0+YRO+8nv8Wb0yGXiDiAkcDjQAKwXkTmGWPis1R7B5hujBktIpWBhUAZT+1a9TBdgAeMMUONMZMzylCgdsa26zLGfGOMqWWMqeVLsgBUrViBIwlJJCQdJzU1lR+WriCq7kNudZJTzpGecUK/jZlGq6aua4ew4kWJ27yNtDQnqWlpxG3exl2lS/p0fG9t2LCFu8uVoXTpSIKDg2ndOpoFCxa71VmwcDEdOj4FQKtWTVixYg0AhQoVZPas8fTv/yHr1m24pu02bZozY8Z8v8a7ccNW7r67NKUy4n2ydVN+WLjUrc6PC5fyTIdWALRo1YiVK1zXjk0aPkO1KvWpVqU+o0dN4NNPRvPt1zEAjBg1hD279zHqy3F+jRen07p4VhvYZ4w5YIy5iutNv0W2OgYomPF3ISAJC1afw6QD4cDhbM+HZWzzu3z5HPT7R3de7PkOTqeTVs0aUu6u0nz57SSqVKxA1F8fYv2mrXz+1QREhJrVqvJOr5cAaBhVl183bqHVs90RgboP1qJ+tmTzF6fTSa+e/Zk7bxIOh4NJk6azc+de3vnnP9i4cRsLFyxh4oTpjBn7KVu3LSc5OYXnnn0FgBf//ix33V2avn1fo2/f1wBoHt2JUxnXak8+1ZQnWz3v93jf7DWQWXPG43A4mBIzg10799L3ndfYvHE7PyxcSszE6Xw1ZhgbtiwlOTmFLp1f99jmQ3Vq0q59K3Zs38XKNa7h4/sDhrH4pxU5D9iLHibrSCbDN8aYbzL+jgCOZtmWADyYrYkBwE8i8gpwO9AAC+LpcwoRaQR8CezNcvBSQDmghzHGcs5T796f+/4b797/+6S+lq+bvzw75IZtiEhroJExpmvG407Ag8aYHlnq9MSVA8NEpA4wFqhqPNwUzeOZNsb8KCIVcHVvf3yKlQisN8bY8z446r9Dzj9wTgSyjscjM57LqgvQyHU4s1ZEbgWKACdv1KjlW1NGtq2zqqeUX6XleNp4PVBeRMriSpR2QPtsdY4AjwETMmZ+bwVO4UHe68vV/wRjfVHveX9j0kSkB7AIcADjjDE7ROQ9IM4YMw/oBXwrIv/ANQHQ2Vh8l0oTRtmTHz7JN8YsxDVVnPW5/ln+jgce9qVNTRhlTznsYXKLJoyyJ/2JslI+0B5GKR9oD6OUD7SHUcp7Jk0TRinvpdvzG1WaMMqedEimlA/0ol8pH2gPo5QP9BpGKe/pLJlSvtAhmVI+0CGZUt4zaTpLppT3dFpZKR9oD6OU93Jz1YWc0IRR9vS/2sPkj6yX24fwuwu75wY6BJ/kr9A80CH4ndFZMqV8kKYJo5TXdFpZKV/YM180YZQ9GZsOyWy3AplS4LrotypWrBZUyqjTVkTiRWSHiPzLqk3tYZQtmRzeWtmbBZVEpDzQF3jYGJMsIsWs2tUeRtlTuhfFM28WVHoBGGmMSQYwxtzwrv1/0IRRtmTSrIuF6y2oFJGtTgWggoisFpF1GesheaRDMmVL6V4MySxWIPNGPqA8UB/X+jErReReY0yKpx2Usp0brwGWpY4rOW6UIN4sqJQA/GKMSQUOisgeXAm0/kbH1CGZsiXjFMtiIXNBJREJwbWgUvZ13Ofg6l0QkSK4hmgHPDWqPYyyJZNumRCe9/duQaVFQEMRiQecwBvGmDOe2tWEUbaUbt2DWPJiQSUD9MwoXtGEUbbkzTVMIGjCKFvyRw+TGzRhlC2lp9lzPkoTRtmSTX+hrAmj7CndqT2MUl7Ti36lfOBM1x5GKa/l9IPL3KIJo2zJrtPKtun3Gj5en21blxO/42d6937pmu0hISFMjhlF/I6f+XnlPEqXjgSgcOE7WLRoGmdO7+Lzz9532yc4OJhRI4eyfdsKtm5ZRsuWjXMt/lXrNxP9t5406fw6Y6Zee5umpBOn6PrmIJ588U2e7/0ex0/95xsY1Rq1p/Xf+9D67314pf/HuRZjw4b12b5tBfHxq3ij98vXbA8JCWHK5FHEx69i1c/z3c7xT4umc/bMbj7/fFBm/b/85VbmzJnItq3L2bxpKR8M6uu3WJ3pQZYlEGzRwwQFBTF8+CCaNG1PQsIx1qyOJTZ2Mbt27c2s83zndqSkpFC5yl9p06Y5HwzqR8dOL3H58hUGDvyEKpXvoUqVe9za7dPnFU6eOkPVe+shIhQufEeuxO90pvPBl+P5Zmg/ShQJpd0rbxNVpyZ3Z7zgAD75ZgrRDf5Ki4b1+GXTdoaPm8qQt1wv2ltCQpj51dBcie0Pmee4iescr12zgNjYn9iZ9Rw/347klHNUrlyXtm2aM/iDfnTo6DrHAwZ+TJUq91ClSkW3dj/77GtWrFhDcHAwi36cyhNPRLFo0bIcx2vXaWVb9DAPPFCd/fsPcfDgEVJTU5k+Yx7R0Q3d6kRHNyRm8kwAZs9eQFTUwwBcuvQ7a9as5/KVK9e0+9xzT/PRR18CrluPnjmTnCvxb9u9j1LhJSgZVpzg4Hw0rleHZWvi3OocOJLAg9WrAlC7ehWWrd2QK7HcyDXnePrc65/jmBkAzJq9gKioukCWc3zZ/Rz//vtlVqxYA0BqaiqbNm8nIiLML/HatYexRcKEh5fgaEJS5uPExGNEhJe4pk5CRh2n08n58xcIDb3zhm0WKlQQgAHvvsG6tQv515TRFCtWJBeih5OnkylRNDTzcfGioZzIlpwV7irNktW/ArB09Xp+u/Q7KecvAHD1aipPv9yPDq/+k6Wrb/hTjByJCA8j4eixzMeJiccJz/bijggvQUKCq47T6eTc+fMez3FWhQoVpGnTBixbtsov8RpjXQLhphNGRJ73sK2biMSJSJzTefFmD5Ej+fI5KBkZztp1cTxUpwm//LKRoUPfCUgsAL27dSBu607adO9D3NadFCtSmKAg1+lfNHkE00YOZmjfHnz01SSOJp0IWJw3w+FwEBMzkpEjx3Hw4BG/tPnf2MMMvNEGY8w3xphaxphaDkd+y4aSko5TMjI883FERBiJScevqROZUcfhcFCwYAGPQ6wzZ5L57bdLzJnzAwCzZsdyf8aQyN+KFbnT7SL+xKkzFM/2zlwstDCfv9uTGaOH8urzTwNQMP/tABQvUhiAkmHFqXVfZXbuO+T3GBOTjhFZ8j89SkRECZISj2Wrc5zISFcdh8NBoYIFvRrGjh71Ifv2HWTEiLF+izfdiGUJBI8JIyJbb1C2AcX9FURc3BbKlStDmTIlCQ4Opm2b5sTGLnarExu7mE4dWwPw5JNNWb58tWW7CxYsoV69OgBERdVl5869FnvcnKr33M3hxOMkHDtJamoaP6xYS/06Nd3qJJ87T3rGIkFjps6l1RP1ATh34SJXr6Zm1tm8Yw93l85+r4acc53jsv85x21bXP8cd2oDwFNenuOBA96gUKGC9Or1rl/jdRqxLIEgntbhEJETwBNA9rcZAdYYY8Kv3cvdLbeW9Gq02eiJKD75ZAAOh4MJE6fx4Ycj6N+/Fxs3bCV2wWJuueUWxo/7nOrVq3L2bAqdnn05s/vfvXsNBQsUICQkmJSU8zRt1oFdu/ZSqlQE48YN545CBTl9+gwvdOvF0aNJFpHc3N37V/66iY9GT8KZnk6rJ+rTrX0rvpw4gyoVyhJVpxY/rfyF4eOmIgI1763E2z2eJyQkmM079jBw+BiCgoT0dEOnVo15snGUT8f29u79jRo9yrBPBhDkCGLihGkM/XAE7/bvzYaNW4iNdZ3jCeOHU616VZLPptCx00uZ53jP7rUULJjlHDdtz/kLFzl4YD27du3lypWrAIwaPYHx47+zjOXqlQSPr/ifS7S2fN389fjMPz1rrBJmLDDeGHPNlZyI/MsY097qAN4mjJ3oche5zyphVpZoY/m6eeT4jD89YTx+DmOM6eJhm2WyKHWz0gI05LJiiw8ulcrOoAmjlNecmjBKec+mP4fRhFH2pD2MUj5IE00Ypbxm188ibPHlS6WySxOxLFa8WYEso95TImJEpJZVm5owypaMF8WTLCuQNQYqA8+ISOXr1CsAvAb84k1cmjDKltLEuljwZgUygPeBD4HL3sSlCaNsKR2xLFl/RpJRsi6uZLkCmYjUAEoaYxZ4G5de9Ctb8uYeGBYLKnkkIkHAp0BnX/bThFG25Mx5E1YrkBUAqgLLxTWBUAKYJyLNjTHuvy/PQhNG2ZIfbkuWuQIZrkRpB2R+YdgYcw7I/M26iCwHentKFtCEUTblxZqwHnm5ApnPNGGULfnj2/1WK5Ble76+N21qwihbymkPk1s0YZQt2fWrMZowypa8+GAyIDRhlC35YVo5V2jCKFuy6WoXmjDKnv5nexix6S/nPLmtfHSgQ/DJbztnBToEv0u36WW/9jDKlv5nexilbobeBEMpH6SJDsmU8poOyZTygV70K+UD7WGU8oH2MEr5QHsYpXxgtIdRyntpmjBKec+pCaOU9/STfqV8oD2MUj7Qi36lfJDmYXXvQNKEUbZkz3TRhFE25bTpZb/evV/ZUhrGslixWlBJRHqKSLyIbBWRpSJS2qpNTRhlS8aL/zzxckGlTUAtY8x9wEzgI6u4NGGULTmNsSwWLBdUMsYsM8Zcyni4Dtcd/j3ShFG2lI6xLBYsF1TKpgvwg1WjetGvbMmbDy4zVhzLuurYNxmLLPlERDoCtYB6VnVt08M8/ng9tm5dxo4dK+nd+6VrtoeEhBATM5IdO1aycuVcSpd29Z6FC9/BokVTOX16J5999p7bPq1bR7N+/SI2blzCoEF9cxzjEw3rs2P7SnbFr+LNN16+boz/mjKaXfGrWLNqfmaMAG+92YNd8avYsX0lDR//z7/Lt98MIylhC5s3LXVr6777KrNq5Tw2bVzCnO8nUKBA/hzHn9WquK1Ev/AWTbu8wdjpsddsTzpxmq59P+Spl97mb28N4fjps27bL176nQadXmfwqEl+jesP3vQwxphvjDG1spSsyWK1oBIAItIAeBtoboy5YhWXLRImKCiI4cMH0aLFc1Sv/hht2zanYsXybnU6d36alJRzVKnyCCNGjMlMgMuXrzBw4DD69PnArX7hwncwZEg/Gjd+hho1GlCiRFGioh7OUYxfDP+AZtEdubdaFE8/3ZJKldxj/Nvzz5CcfI6Klevy+RffMmTw2wBUqlSetm1bcF/1R2narAMjvhhMUJDr1E+aNJ2mzTpcc7yvv/qYfm8P5v4aDZgz5wd69+p+07Fn53SmM3jUJEa/14s5Xw3hhxXr2H/E/bU0bOxUoh97mFmjPuDFZ1rwxfgZbtu/nDSLmlXv8VtM18SY82uYzAWVRCQE14JKbmvCiMj9wNe4kuWkN3HZImEeeKA6+/cf4uDBI6SmpjJjxnyioxu61YmObsjkyTMBmD17YeaL/9Kl31mzZj1Xrrgvglu2bCn27TvE6Yx3xn//exUtWza+6RhrP3C/W4zTp8+lefQTbnWaRzckJsb1wpo1awGPRtXNeP4Jpk+fy9WrVzl06Cj79x+i9gP3A/Dzql84m5xyzfEqlL+LlT+vA2DJ0p9p1arJTcee3fY9BygVXpzIsGIEB+ej0SMPsmztRrc6B44k8mC1Sq7/92qVWLbuP9vj9x7kbMp5/q9GVb/FlJ2TdMviiTEmDfhjQaWdwPQ/FlQSkeYZ1T4G8gMzRGSziFgusmSZMCJSUUQeE5H82Z5vZLWvt8LDS5CQkJT5ODHxGOHhxW9Yx+l0cv78BUJD77xhm/v3H6Z8+bsoXToSh8NBdHRDIiPDbz7GiBIczRJjQuIxwsNL3LCO0+nk3LnzhIbeSXj4dfaNcN83u/j4PTRv7krI1k81o2QOYs/uxJlkihcpnPm4eJHCnDyT7FanQtlSLFm9AYClazbw2++XSTl/kfT0dD4ZM5WeXdv5LZ7rMcZYFi/aWGiMqWCMudsY80HGc/3/WH3MGNPAGFPcGFM9ozT33KJFwojIq8Bc4BVgu4hknZYbbBlxAKWknOPVV98mJmYkS5fO5PDhBJxOu/7w9Vpdu/Wk+4vP8cu6HyhQ4HauXk39U4/fq2s7NmzfRdse/yRu2y6Khd5JUJAwbcFS6ta6jxJZEi435LSHyS1Ws2QvADWNMRdFpAwwU0TKGGOGw41vmpx19iJfvjtxODxfsCYlHXd794+ICCMp6cR16yQmHsfhcFCwYAHOZHtXzG7hwiUsXLgEgC5d2uN03vxJTko87vYuHxkRRlLS8evWSUw8hsPhoFChgpw5k0xS0nX2TXTfN7vdu/fTuKlrDdPy5e+iSePHbjr27IqH3smJLBfxJ06fpVi23rpY6J189s6rAFz6/TJLVsdRMP/tbNm5n407djN9wb+5dPkyqalp3PaXW3n9+bZ+iw8g3aZfvrQakgUZYy4CGGMOAfWBxiLyKR4SJuvshVWyAMTFbaFcubKUKVOS4OBg2rSJJjZ2sVud2NjFdOzYGoAnn2zC8uVrLNstWjQUgDvuKES3bp0YP/47y31uZH3cZrcY27ZtwfzYn9zqzI/9iU6d2gDw1FNNWbZ8debzbdu2ICQkhDJlSlKuXFl+Xb/Jq9hFhH59X+Prb2JuOvbsqlQoy+GkEyQcP0Vqaho/rvyF+g/d71Yn+dwF0tNdbzBjpsfSquEjAAx98+/8NPEzfpwwjF5d2hH92MN+TxZwTStblUCw6mFOiEh1Y8xmgIyephkwDrjXX0E4nU5ef/2fzJ8fg8PhYOLEaezcuYf+/XuyYcM2FixYzIQJ0xg37nN27FjJ2bMpPPtsj8z9d+9eTYECBQgJCSY6+gmaNevIrl17GTZsAPfe6/o2xODBn7Nv38Ecxfja6++wcMG/cAQFMWHiNOLj9zDg3d7EbdhCbOxixo2fysQJX7ArfhXJySm07+iaHo+P38PMmfPZtmUZaU4nr772duaLcXLMSOo9UociRQpz6EAcA9/7hPETptLu6ZZ0794ZgDlzFjJh4rSbjj27fA4H/bp3ovs7H+NMT6dlw0coVzqSkTGzqVy+DFEP1WD9tl18MWEGAtSoeg9vv/ys347vDbveZkk8XTyJSCSQZoy5ZvwgIg8bY1ZbHeDWW0vZ8//cg7T0vHOtA3lzuYtb7n7I4zootcPrWb5ufk1a8aevpeKxhzHGJHjYZpksSt0s/cWlUj5wGnv+HkYTRtmSJoxSPtAhmVI+0B5GKR/Y9YNLTRhlS9rDKOUDvYZRygfawyjlA6ex57ctNGGULXnze5dA0IRRtqRDMqV8oNPKSvkgXXsYpbxn19/DaMIoW3Kmaw+jlNf0ol8pH+i0slI+0B5GKR/otLJSPtBpZaV8oNcwSvnArkMyj/clszsR6XYzC+gESl6LF/JmzLnJFstd5EA36yq2ktfihbwZc67J6wmj1J9KE0YpH+T1hMlrY+u8Fi/kzZhzTZ6+6Ffqz5bXexil/lR5MmFEpJGI7BaRfSLSJ9DxWBGRcSJyUkS2BzoWb4hISRFZJiLxIrJDRF4LdEx2keeGZCLiAPYAjwMJuJaXfsYYEx/QwDwQkUeAi8AkY0zuLT3sJyISBoQZYzaKSAFgA9DSzuf4z5IXe5jawD5jzAFjzFVgKtDCYp+AMsasBM5aVrQJY8wxY8zGjL8v4Fq2OyKwUdlDXkyYCOBolscJ6D9mrslYDPh+4JcAh2ILeTFh1J9ERPIDs4DXjTHnAx2PHeTFhEkESmZ5HJnxnPIjEQnGlSxTjDGzAx2PXeTFhFkPlBeRsiISArQD5gU4pv8qIiLAWGCnMebTQMdjJ3kuYYwxaUAPYBGui9HpxpgdgY3KMxH5DlgL3CMiCSLSJdAxWXgY6AQ8KiKbM0qTQAdlB3luWlmpQMpzPYxSgaQJo5QPNGGU8oEmjFI+0IRRygeaMEr5QBNGKR9owijlg/8HpOoY8n2Ff6oAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(df['label'], df['BERT_label'], digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFYK-FXdy3qY",
        "outputId": "14f814dc-a432-4e14-b61a-54f9566de2e7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.965     0.976     0.970      1344\n",
            "           1      0.970     0.948     0.959       517\n",
            "           2      0.946     0.937     0.941       429\n",
            "\n",
            "    accuracy                          0.962      2290\n",
            "   macro avg      0.960     0.954     0.957      2290\n",
            "weighted avg      0.962     0.962     0.962      2290\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROBERTA"
      ],
      "metadata": {
        "id": "GYS1LMinzazG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(df['label'], df['ROBERTA_label'])"
      ],
      "metadata": {
        "id": "Nc4MdYAXzcnW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1), index = [i for i in range(3)],\n",
        "                     columns = [i for i in range(3)])"
      ],
      "metadata": {
        "id": "WHuCyb11zeqA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (3,3))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "lN6EauDRzfk6",
        "outputId": "43cf5b14-ce99-43b1-849e-8ecb2ab488a9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(df['label'], df['ROBERTA_label'], digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GELFLRPYzhzo",
        "outputId": "a5594e7d-b3e8-47bf-8d65-dc9f9fb3c115"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.968     0.974     0.971      1344\n",
            "           1      0.969     0.965     0.967       517\n",
            "           2      0.943     0.930     0.937       429\n",
            "\n",
            "    accuracy                          0.964      2290\n",
            "   macro avg      0.960     0.956     0.958      2290\n",
            "weighted avg      0.964     0.964     0.964      2290\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_path  = './data/capstone/CLAWS/covidhate/annotated_tweets_w_classification.csv'\n",
        "df.to_csv(df_path, index=False)"
      ],
      "metadata": {
        "id": "Ft9gnro05xv7"
      },
      "execution_count": 54,
      "outputs": []
    }
  ]
}